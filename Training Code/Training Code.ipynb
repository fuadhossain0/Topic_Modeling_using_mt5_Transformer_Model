{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6p18wfDSxqg"
      },
      "outputs": [],
      "source": [
        "!pip install datasets evaluate transformers[sentencepiece]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THk7msYGztbA"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWr_xtDUIHHP"
      },
      "outputs": [],
      "source": [
        "root = \"/content/drive/MyDrive/Thesis Final/Working Files/save model temp\"\n",
        "import os\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dv3OQ3XelRl0"
      },
      "outputs": [],
      "source": [
        "st_chunk= 8\n",
        "shard_chunk=15\n",
        "num_shards = 20"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "path = \"/content/drive/MyDrive/Thesis Final/Working Files/Dataset/\"\n",
        "news_file = path + \"Dataset_4.json\""
      ],
      "metadata": {
        "id": "FmEheJwmrBMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_list = []\n",
        "with open(news_file, \"r\", encoding=\"utf-8\") as fp:\n",
        "  news_list = json.load(fp)"
      ],
      "metadata": {
        "id": "lETqJuXoGRAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "all_contents = Dataset.from_list(news_list)"
      ],
      "metadata": {
        "id": "j-dvSDgXGew1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_contents\n",
        "print(type(all_contents))"
      ],
      "metadata": {
        "id": "PqQvDeVeGmie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shard_data = {i: list(range(i * len(all_contents) // num_shards, (i + 1) * len(all_contents) // num_shards)) for i in range(num_shards)}"
      ],
      "metadata": {
        "id": "78wRC-JzH1qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "selected_chunk_data = all_contents.select(shard_data[shard_chunk])\n",
        "print(f\"Loaded chunk: {st_chunk}, shard: {shard_chunk}, total items: {len(selected_chunk_data)}\")"
      ],
      "metadata": {
        "id": "n3VzpjTsIAZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5W5cH2UYjErV"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "tokenizer_checkpoint = os.path.join(root, \"tokenizer\")\n",
        "model_checkpoint = \"text_summarization_model\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(tokenizer_checkpoint)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(os.path.join(root, \"pretrained model\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HVwhx8_Hif_N"
      },
      "outputs": [],
      "source": [
        "max_input_length, max_target_length = 512, 30\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = tokenizer(examples[\"content\"], max_length=max_input_length, truncation=True)\n",
        "    labels = tokenizer(examples[\"title\"], max_length=max_target_length, truncation=True)\n",
        "    inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    inputs[\"labels_mask\"] = labels[\"attention_mask\"]\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "csGjyeRRia4P"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = selected_chunk_data.train_test_split(train_size=0.9, seed=20).map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cam5yUmmiYN3"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "batch_size, num_train_epochs = 3, 8\n",
        "\n",
        "logging_steps = len(tokenized_datasets[\"train\"]) // batch_size\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=os.path.join(root, \"model\"),\n",
        "    evaluation_strategy=\"no\",\n",
        "    learning_rate=5.6e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    logging_steps=logging_steps,\n",
        "    predict_with_generate=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHFh43ChQHzH"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "import gc\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"train\", \"test\"].column_names)\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9rlvlIallRsI"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTgNA1FhlRwK"
      },
      "outputs": [],
      "source": [
        "trainer.train()\n",
        "model.save_pretrained(os.path.join(root, \"saved model\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ev1z-COfN8NZ"
      },
      "outputs": [],
      "source": [
        "## Halting\n",
        "assert(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8S5oASpNJ2W"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "save_model=root+\"/saved model\"\n",
        "model=AutoModelForSeq2SeqLM.from_pretrained(save_model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}